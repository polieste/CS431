{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "metadata": {
        "id": "AOpGoE2T-YXS"
      },
      "cell_type": "markdown",
      "source": [
        "##### Copyright 2018 The TensorFlow Authors.\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\").\n",
        "\n",
        "# Neural Machine Translation with Attention\n",
        "\n",
        "<table class=\"tfo-notebook-buttons\" align=\"left\"><td>\n",
        "<a target=\"_blank\"  href=\"https://colab.sandbox.google.com/github/tensorflow/tensorflow/blob/master/tensorflow/contrib/eager/python/examples/nmt_with_attention/nmt_with_attention.ipynb\">\n",
        "    <img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" /><span>Run in Google Colab</span></a>  \n",
        "</td><td>\n",
        "<a target=\"_blank\"  href=\"https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/eager/python/examples/nmt_with_attention/nmt_with_attention.ipynb\"><img width=32px src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" /><span>View source on GitHub</span></a></td></table>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zPINjWvVY0sW",
        "outputId": "f8d31150-7579-4e1b-fb89-e4d19ccbf95f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "CiwtNgENbx2g"
      },
      "cell_type": "markdown",
      "source": [
        "This notebook trains a sequence to sequence (seq2seq) model for Spanish to English translation using [tf.keras](https://www.tensorflow.org/programmers_guide/keras) and [eager execution](https://www.tensorflow.org/programmers_guide/eager). This is an advanced example that assumes some knowledge of sequence to sequence models.\n",
        "\n",
        "After training the model in this notebook, you will be able to input a Spanish sentence, such as *\"¿todavia estan en casa?\"*, and return the English translation: *\"are you still at home?\"*\n",
        "\n",
        "The translation quality is reasonable for a toy example, but the generated attention plot is perhaps more interesting. This shows which parts of the input sentence has the model's attention while translating:\n",
        "\n",
        "<img src=\"https://tensorflow.org/images/spanish-english.png\" alt=\"spanish-english attention plot\">\n",
        "\n",
        "Note: This example takes approximately 10 mintues to run on a single P100 GPU."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install underthesea==1.3.5"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YK9D5BKx8hdT",
        "outputId": "6f30291d-6b48-4d6d-ad4f-bff418ed1358"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting underthesea==1.3.5\n",
            "  Downloading underthesea-1.3.5-py3-none-any.whl (11.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 11.0 MB 4.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from underthesea==1.3.5) (4.64.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.8/dist-packages (from underthesea==1.3.5) (6.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.8/dist-packages (from underthesea==1.3.5) (1.0.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from underthesea==1.3.5) (1.2.0)\n",
            "Collecting python-crfsuite>=0.9.6\n",
            "  Downloading python_crfsuite-0.9.8-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0 MB 72.2 MB/s \n",
            "\u001b[?25hCollecting unidecode\n",
            "  Downloading Unidecode-1.3.6-py3-none-any.whl (235 kB)\n",
            "\u001b[K     |████████████████████████████████| 235 kB 86.2 MB/s \n",
            "\u001b[?25hCollecting underthesea-core==0.0.5a2\n",
            "  Downloading underthesea_core-0.0.5_alpha.2-cp38-cp38-manylinux2010_x86_64.whl (591 kB)\n",
            "\u001b[K     |████████████████████████████████| 591 kB 76.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: nltk in /usr/local/lib/python3.8/dist-packages (from underthesea==1.3.5) (3.7)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from underthesea==1.3.5) (2.23.0)\n",
            "Requirement already satisfied: Click>=6.0 in /usr/local/lib/python3.8/dist-packages (from underthesea==1.3.5) (7.1.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.8/dist-packages (from nltk->underthesea==1.3.5) (2022.6.2)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->underthesea==1.3.5) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->underthesea==1.3.5) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->underthesea==1.3.5) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->underthesea==1.3.5) (2022.12.7)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->underthesea==1.3.5) (1.7.3)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->underthesea==1.3.5) (3.1.0)\n",
            "Requirement already satisfied: numpy>=1.14.6 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->underthesea==1.3.5) (1.21.6)\n",
            "Installing collected packages: unidecode, underthesea-core, python-crfsuite, underthesea\n",
            "Successfully installed python-crfsuite-0.9.8 underthesea-1.3.5 underthesea-core-0.0.5a2 unidecode-1.3.6\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "tnxXKDjq3jEL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f17ba097-69fa-4e79-fd7b-a96780a131b3"
      },
      "cell_type": "code",
      "source": [
        "from __future__ import absolute_import, division, print_function\n",
        "\n",
        "# Import TensorFlow >= 1.9 and enable eager execution\n",
        "import tensorflow as tf\n",
        "\n",
        "#tf.enable_eager_execution()\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import unicodedata\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import time\n",
        "from underthesea import word_tokenize\n",
        "print(tf.__version__)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.9.2\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "wfodePkj3jEa"
      },
      "cell_type": "markdown",
      "source": [
        "## Download and prepare the dataset\n",
        "\n",
        "We'll use a language dataset provided by http://www.manythings.org/anki/. This dataset contains language translation pairs in the format:\n",
        "\n",
        "```\n",
        "May I borrow this book?\t¿Puedo tomar prestado este libro?\n",
        "```\n",
        "\n",
        "There are a variety of languages available, but we'll use the English-Spanish dataset. For convenience, we've hosted a copy of this dataset on Google Cloud, but you can also download your own copy. After downloading the dataset, here are the steps we'll take to prepare the data:\n",
        "\n",
        "1. Add a *start* and *end* token to each sentence.\n",
        "2. Clean the sentences by removing special characters.\n",
        "3. Create a word index and reverse word index (dictionaries mapping from word → id and id → word).\n",
        "4. Pad each sentence to a maximum length."
      ]
    },
    {
      "metadata": {
        "id": "kRVATYOgJs1b"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "path_to_file =\"/content/drive/MyDrive/CS431/vie_eng/vie.txt\""
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/CS431/vie_eng/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9XdoNbTAZUh7",
        "outputId": "8ed21f1b-10da-4f80-fc6c-8147f4f72261"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/CS431/vie_eng\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import preprocessing\n",
        "lines = pd.read_table( 'vie.txt' , names=[ 'eng' , 'vie' ] )\n",
        "lines.reset_index( level=0 , inplace=True )\n",
        "lines.rename( columns={ 'index' : 'eng' , 'eng' : 'vie' , 'vie' : 'c' } , inplace=True )\n",
        "lines = lines.drop( 'c' , 1 )\n",
        "lines = lines.iloc[4000:8081] \n",
        "lines.head()\n",
        "\n",
        "eng_lines = list()\n",
        "for line in lines.eng:\n",
        "    eng_lines.append( '<START>'+line+'<END>' ) \n",
        "\n",
        "tokenizer = preprocessing.text.Tokenizer()\n",
        "tokenizer.fit_on_texts( eng_lines ) \n",
        "tokenized_eng_lines = tokenizer.texts_to_sequences( eng_lines ) \n",
        "\n",
        "length_list = list()\n",
        "for token_seq in tokenized_eng_lines:\n",
        "    length_list.append( len( token_seq ))\n",
        "max_input_length = np.array( length_list ).max()\n",
        "print( 'English max length is {}'.format( max_input_length ))\n",
        "\n",
        "padded_eng_lines = preprocessing.sequence.pad_sequences( tokenized_eng_lines , maxlen=max_input_length , padding='post' )\n",
        "encoder_input_data = np.array( padded_eng_lines )\n",
        "print( 'Encoder input data shape -> {}'.format( encoder_input_data.shape ))\n",
        "\n",
        "eng_word_dict = tokenizer.word_index\n",
        "num_eng_tokens = len( eng_word_dict )+1\n",
        "print( 'Number of English tokens = {}'.format( num_eng_tokens))\n",
        "\n",
        "vie_tokenize=list()\n",
        "for i in lines.vie:\n",
        "  vie_tokenize.append(['<START>']+word_tokenize(i)+['<END>'])\n",
        "tokenizer = preprocessing.text.Tokenizer()\n",
        "tokenizer.fit_on_texts(vie_tokenize)\n",
        "tokenized_vie_lines = tokenizer.texts_to_sequences( vie_tokenize ) \n",
        "\n",
        "length_list = list()\n",
        "for token_seq in tokenized_vie_lines:\n",
        "    length_list.append( len( token_seq ))\n",
        "max_output_length = np.array( length_list ).max()\n",
        "print( 'Viet max length is {}'.format( max_output_length ))\n",
        "\n",
        "padded_vie_lines = preprocessing.sequence.pad_sequences( tokenized_vie_lines , maxlen=max_output_length, padding='post' )\n",
        "decoder_input_data = np.array( padded_vie_lines )\n",
        "print( 'Decoder input data shape -> {}'.format( decoder_input_data.shape ))\n",
        "\n",
        "vie_word_dict = tokenizer.word_index\n",
        "num_vie_tokens = len( vie_word_dict )+1\n",
        "print( 'Number of vie tokens = {}'.format( num_vie_tokens))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m2eKzUxW-y_F",
        "outputId": "5951ee9a-0598-4f6f-e3fd-9d9cb4ed40a8"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-0e5551905753>:5: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
            "  lines = lines.drop( 'c' , 1 )\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "English max length is 34\n",
            "Encoder input data shape -> (4081, 34)\n",
            "Number of English tokens = 3095\n",
            "Viet max length is 38\n",
            "Decoder input data shape -> (4081, 38)\n",
            "Number of vie tokens = 2930\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "eAY9k49G3jE_"
      },
      "cell_type": "code",
      "source": [
        "def max_length(tensor):\n",
        "    return max(len(t) for t in tensor)\n",
        "\n",
        "\n",
        "def load_dataset(path, num_examples):\n",
        "    # creating cleaned input, output pairs\n",
        "    #pairs = create_dataset(path, num_examples)\n",
        "\n",
        "    # index language using the class defined above    \n",
        "    inp_lang = eng_word_dict\n",
        "    targ_lang = vie_word_dict\n",
        "    \n",
        "    # Vectorize the input and target languages\n",
        "    \n",
        "    # English sentences\n",
        "    input_tensor = tokenized_eng_lines\n",
        "    \n",
        "    # Vie sentences\n",
        "    target_tensor = tokenized_vie_lines\n",
        "    \n",
        "    # Calculate max_length of input and output tensor\n",
        "    # Here, we'll set those to the longest sentence in the dataset\n",
        "    max_length_inp, max_length_tar = max_length(input_tensor), max_length(target_tensor)\n",
        "    \n",
        "    # Padding the input and output tensor to the maximum length\n",
        "    input_tensor = tf.keras.preprocessing.sequence.pad_sequences(input_tensor, \n",
        "                                                                 maxlen=max_length_inp,\n",
        "                                                                 padding='post')\n",
        "    \n",
        "    target_tensor = tf.keras.preprocessing.sequence.pad_sequences(target_tensor, \n",
        "                                                                  maxlen=max_length_tar, \n",
        "                                                                  padding='post')\n",
        "    \n",
        "    return input_tensor, target_tensor, inp_lang, targ_lang, max_length_inp, max_length_tar"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GOi42V79Ydlr"
      },
      "cell_type": "markdown",
      "source": [
        "### Limit the size of the dataset to experiment faster (optional)\n",
        "\n",
        "Training on the complete dataset of >100,000 sentences will take a long time. To train faster, we can limit the size of the dataset to 30,000 sentences (of course, translation quality degrades with less data):"
      ]
    },
    {
      "metadata": {
        "id": "cnxC7q-j3jFD"
      },
      "cell_type": "code",
      "source": [
        "# Try experimenting with the size of that dataset\n",
        "num_examples = 30000\n",
        "input_tensor, target_tensor, inp_lang, targ_lang, max_length_inp, max_length_targ = load_dataset(path_to_file, num_examples)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4QILQkOs3jFG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1fe84f15-e70f-4230-9730-27d40eb0cfc8"
      },
      "cell_type": "code",
      "source": [
        "# Creating training and validation sets using an 80-20 split\n",
        "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2)\n",
        "\n",
        "# Show length\n",
        "len(input_tensor_train), len(target_tensor_train), len(input_tensor_val), len(target_tensor_val)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3264, 3264, 817, 817)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "metadata": {
        "id": "rgCLkfv5uO3d"
      },
      "cell_type": "markdown",
      "source": [
        "### Create a tf.data dataset"
      ]
    },
    {
      "metadata": {
        "id": "TqHsArVZ3jFS"
      },
      "cell_type": "code",
      "source": [
        "BUFFER_SIZE = len(input_tensor_train)\n",
        "BATCH_SIZE = 64\n",
        "embedding_dim = 256\n",
        "units = 1024\n",
        "vocab_inp_size = len(inp_lang)\n",
        "vocab_tar_size = len(targ_lang)\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
        "dataset = dataset.batch(BATCH_SIZE)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TNfHIF71ulLu"
      },
      "cell_type": "markdown",
      "source": [
        "## Write the encoder and decoder model\n",
        "\n",
        "Here, we'll implement an encoder-decoder model with attention which you can read about in the TensorFlow [Neural Machine Translation (seq2seq) tutorial](https://www.tensorflow.org/tutorials/seq2seq). This example uses a more recent set of APIs. This notebook implements the [attention equations](https://www.tensorflow.org/tutorials/seq2seq#background_on_the_attention_mechanism) from the seq2seq tutorial. The following diagram shows that each input words is assigned a weight by the attention mechanism which is then used by the decoder to predict the next word in the sentence.\n",
        "\n",
        "<img src=\"https://www.tensorflow.org/images/seq2seq/attention_mechanism.jpg\" width=\"500\" alt=\"attention mechanism\">\n",
        "\n",
        "The input is put through an encoder model which gives us the encoder output of shape *(batch_size, max_length, hidden_size)* and the encoder hidden state of shape *(batch_size, hidden_size)*. \n",
        "\n",
        "Here are the equations that are implemented:\n",
        "\n",
        "<img src=\"https://www.tensorflow.org/images/seq2seq/attention_equation_0.jpg\" alt=\"attention equation 0\" width=\"800\">\n",
        "<img src=\"https://www.tensorflow.org/images/seq2seq/attention_equation_1.jpg\" alt=\"attention equation 1\" width=\"800\">\n",
        "\n",
        "We're using *Bahdanau attention*. Lets decide on notation before writing the simplified form:\n",
        "\n",
        "* FC = Fully connected (dense) layer\n",
        "* EO = Encoder output\n",
        "* H = hidden state\n",
        "* X = input to the decoder\n",
        "\n",
        "And the pseudo-code:\n",
        "\n",
        "* `score = FC(tanh(FC(EO) + FC(H)))`\n",
        "* `attention weights = softmax(score, axis = 1)`. Softmax by default is applied on the last axis but here we want to apply it on the *1st axis*, since the shape of score is *(batch_size, max_length, hidden_size)*. `Max_length` is the length of our input. Since we are trying to assign a weight to each input, softmax should be applied on that axis.\n",
        "* `context vector = sum(attention weights * EO, axis = 1)`. Same reason as above for choosing axis as 1.\n",
        "* `embedding output` = The input to the decoder X is passed through an embedding layer.\n",
        "* `merged vector = concat(embedding output, context vector)`\n",
        "* This merged vector is then given to the GRU\n",
        "  \n",
        "The shapes of all the vectors at each step have been specified in the comments in the code:"
      ]
    },
    {
      "metadata": {
        "id": "avyJ_4VIUoHb"
      },
      "cell_type": "code",
      "source": [
        "def gru(units):\n",
        "  # If you have a GPU, we recommend using CuDNNGRU(provides a 3x speedup than GRU)\n",
        "  # the code automatically does that.\n",
        "  if tf.test.is_gpu_available():\n",
        "    return tf.keras.layers.LSTM(units, \n",
        "                                    return_sequences=True, \n",
        "                                    return_state=True, \n",
        "                                    recurrent_initializer='glorot_uniform')\n",
        "  else:\n",
        "    return tf.keras.layers.GRU(units, \n",
        "                               return_sequences=True, \n",
        "                               return_state=True, \n",
        "                               recurrent_activation='sigmoid', \n",
        "                               recurrent_initializer='glorot_uniform')"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nZ2rI24i3jFg"
      },
      "cell_type": "code",
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.batch_sz = batch_sz\n",
        "        self.enc_units = enc_units\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "        self.gru = gru(self.enc_units)\n",
        "        \n",
        "    def call(self, x, hidden):\n",
        "        x = self.embedding(x)\n",
        "        output, state ,_= self.gru(x, initial_state = hidden)        \n",
        "        return output, state\n",
        "    \n",
        "    def initialize_hidden_state(self):\n",
        "        dim=tf.zeros((self.batch_sz, self.enc_units))\n",
        "        return [dim,dim]"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yJ_B3mhW3jFk"
      },
      "cell_type": "code",
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.batch_sz = batch_sz\n",
        "        self.dec_units = dec_units\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "        self.gru = gru(self.dec_units)\n",
        "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
        "        \n",
        "        # used for attention\n",
        "        self.W1 = tf.keras.layers.Dense(self.dec_units)\n",
        "        self.W2 = tf.keras.layers.Dense(self.dec_units)\n",
        "        self.V = tf.keras.layers.Dense(1)\n",
        "        \n",
        "    def call(self, x, hidden, enc_output):\n",
        "        # enc_output shape == (batch_size, max_length, hidden_size)\n",
        "        \n",
        "        # hidden shape == (batch_size, hidden size)\n",
        "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
        "        # we are doing this to perform addition to calculate the score\n",
        "        hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
        "        \n",
        "        # score shape == (batch_size, max_length, hidden_size)\n",
        "        score = tf.nn.tanh(self.W1(enc_output) + self.W2(hidden_with_time_axis))\n",
        "        \n",
        "        # attention_weights shape == (batch_size, max_length, 1)\n",
        "        # we get 1 at the last axis because we are applying score to self.V\n",
        "        attention_weights = tf.nn.softmax(self.V(score), axis=1)\n",
        "        \n",
        "        # context_vector shape after sum == (batch_size, hidden_size)\n",
        "        context_vector = attention_weights * enc_output\n",
        "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "        \n",
        "        # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
        "        x = self.embedding(x)\n",
        "        \n",
        "        # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
        "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
        "        \n",
        "        # passing the concatenated vector to the GRU\n",
        "        output, state,_ = self.gru(x)\n",
        "        \n",
        "        # output shape == (batch_size * max_length, hidden_size)\n",
        "        output = tf.reshape(output, (-1, output.shape[2]))\n",
        "        \n",
        "        # output shape == (batch_size * max_length, vocab)\n",
        "        x = self.fc(output)\n",
        "        \n",
        "        return x, state, attention_weights\n",
        "    def initialize_hidden_state(self):\n",
        "        dim=tf.zeros((self.batch_sz, self.enc_units))\n",
        "        return [dim,dim]"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "metadata": {
        "id": "P5UY8wko3jFp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95fab64d-acf7-4e48-e3c8-38fee2d559a7"
      },
      "cell_type": "code",
      "source": [
        "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
        "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From <ipython-input-12-3429e4dd8175>:4: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.config.list_physical_devices('GPU')` instead.\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "_ch_71VbIRfK"
      },
      "cell_type": "markdown",
      "source": [
        "## Define the optimizer and the loss function"
      ]
    },
    {
      "metadata": {
        "id": "WmTHr5iV3jFr"
      },
      "cell_type": "code",
      "source": [
        "optimizer = tf.optimizers.Adam()\n",
        "\n",
        "\n",
        "def loss_function(real, pred):\n",
        "  mask = 1 - np.equal(real, 0)\n",
        "  loss_ = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=real, logits=pred) * mask\n",
        "  return tf.reduce_mean(loss_)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hpObfY22IddU"
      },
      "cell_type": "markdown",
      "source": [
        "## Training\n",
        "\n",
        "1. Pass the *input* through the *encoder* which return *encoder output* and the *encoder hidden state*.\n",
        "2. The encoder output, encoder hidden state and the decoder input (which is the *start token*) is passed to the decoder.\n",
        "3. The decoder returns the *predictions* and the *decoder hidden state*.\n",
        "4. The decoder hidden state is then passed back into the model and the predictions are used to calculate the loss.\n",
        "5. Use *teacher forcing* to decide the next input to the decoder.\n",
        "6. *Teacher forcing* is the technique where the *target word* is passed as the *next input* to the decoder.\n",
        "7. The final step is to calculate the gradients and apply it to the optimizer and backpropagate."
      ]
    },
    {
      "metadata": {
        "id": "ddefjBMa3jF0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb7c176e-43f1-40c3-ad8c-a612e2cddf3c"
      },
      "cell_type": "code",
      "source": [
        "EPOCHS = 50\n",
        "for epoch in range(EPOCHS):\n",
        "    start = time.time()\n",
        "    \n",
        "    hidden = encoder.initialize_hidden_state()\n",
        "    total_loss = 0\n",
        "    \n",
        "    for (batch, (inp, targ)) in enumerate(dataset):\n",
        "        loss = 0\n",
        "        \n",
        "        with tf.GradientTape() as tape:\n",
        "            enc_output, enc_hidden = encoder(inp, hidden)\n",
        "            \n",
        "            dec_hidden = enc_hidden\n",
        "            \n",
        "            dec_input = tf.expand_dims([2] * BATCH_SIZE, 1)       \n",
        "            \n",
        "            # Teacher forcing - feeding the target as the next input\n",
        "            for t in range(1, targ.shape[1]):\n",
        "                # passing enc_output to the decoder\n",
        "                predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
        "                \n",
        "                loss += loss_function(targ[:, t], predictions)\n",
        "                \n",
        "                # using teacher forcing\n",
        "                dec_input = tf.expand_dims(targ[:, t], 1)\n",
        "        \n",
        "        total_loss += (loss / int(targ.shape[1]))\n",
        "        \n",
        "        variables = encoder.variables + decoder.variables\n",
        "        \n",
        "        gradients = tape.gradient(loss, variables)\n",
        "      \n",
        "        optimizer.apply_gradients(zip(gradients, variables), tf.compat.v1.train.get_or_create_global_step())\n",
        "\n",
        "        if batch % 100 == 0:\n",
        "            print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                                         batch,\n",
        "                                                         loss.numpy() / int(targ.shape[1])))\n",
        "    \n",
        "    print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                        total_loss/len(input_tensor)))\n",
        "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 Batch 0 Loss 1.6652\n",
            "Epoch 1 Loss 0.0199\n",
            "Time taken for 1 epoch 40.94463109970093 sec\n",
            "\n",
            "Epoch 2 Batch 0 Loss 1.4897\n",
            "Epoch 2 Loss 0.0182\n",
            "Time taken for 1 epoch 34.70219540596008 sec\n",
            "\n",
            "Epoch 3 Batch 0 Loss 1.3902\n",
            "Epoch 3 Loss 0.0168\n",
            "Time taken for 1 epoch 35.751349210739136 sec\n",
            "\n",
            "Epoch 4 Batch 0 Loss 1.1573\n",
            "Epoch 4 Loss 0.0157\n",
            "Time taken for 1 epoch 34.69684052467346 sec\n",
            "\n",
            "Epoch 5 Batch 0 Loss 1.1487\n",
            "Epoch 5 Loss 0.0147\n",
            "Time taken for 1 epoch 37.02226972579956 sec\n",
            "\n",
            "Epoch 6 Batch 0 Loss 1.0862\n",
            "Epoch 6 Loss 0.0137\n",
            "Time taken for 1 epoch 36.6110417842865 sec\n",
            "\n",
            "Epoch 7 Batch 0 Loss 1.0938\n",
            "Epoch 7 Loss 0.0128\n",
            "Time taken for 1 epoch 36.37492561340332 sec\n",
            "\n",
            "Epoch 8 Batch 0 Loss 0.9325\n",
            "Epoch 8 Loss 0.0120\n",
            "Time taken for 1 epoch 36.37864685058594 sec\n",
            "\n",
            "Epoch 9 Batch 0 Loss 0.9412\n",
            "Epoch 9 Loss 0.0113\n",
            "Time taken for 1 epoch 36.368976354599 sec\n",
            "\n",
            "Epoch 10 Batch 0 Loss 0.9533\n",
            "Epoch 10 Loss 0.0106\n",
            "Time taken for 1 epoch 35.11941194534302 sec\n",
            "\n",
            "Epoch 11 Batch 0 Loss 0.6820\n",
            "Epoch 11 Loss 0.0099\n",
            "Time taken for 1 epoch 34.10835361480713 sec\n",
            "\n",
            "Epoch 12 Batch 0 Loss 0.7074\n",
            "Epoch 12 Loss 0.0092\n",
            "Time taken for 1 epoch 34.25027251243591 sec\n",
            "\n",
            "Epoch 13 Batch 0 Loss 0.7492\n",
            "Epoch 13 Loss 0.0085\n",
            "Time taken for 1 epoch 34.15167570114136 sec\n",
            "\n",
            "Epoch 14 Batch 0 Loss 0.6285\n",
            "Epoch 14 Loss 0.0078\n",
            "Time taken for 1 epoch 34.35783123970032 sec\n",
            "\n",
            "Epoch 15 Batch 0 Loss 0.4998\n",
            "Epoch 15 Loss 0.0071\n",
            "Time taken for 1 epoch 34.46663999557495 sec\n",
            "\n",
            "Epoch 16 Batch 0 Loss 0.4699\n",
            "Epoch 16 Loss 0.0065\n",
            "Time taken for 1 epoch 34.843950271606445 sec\n",
            "\n",
            "Epoch 17 Batch 0 Loss 0.4292\n",
            "Epoch 17 Loss 0.0059\n",
            "Time taken for 1 epoch 34.0272650718689 sec\n",
            "\n",
            "Epoch 18 Batch 0 Loss 0.4190\n",
            "Epoch 18 Loss 0.0053\n",
            "Time taken for 1 epoch 34.190062522888184 sec\n",
            "\n",
            "Epoch 19 Batch 0 Loss 0.3752\n",
            "Epoch 19 Loss 0.0047\n",
            "Time taken for 1 epoch 34.111045122146606 sec\n",
            "\n",
            "Epoch 20 Batch 0 Loss 0.3027\n",
            "Epoch 20 Loss 0.0042\n",
            "Time taken for 1 epoch 33.84925556182861 sec\n",
            "\n",
            "Epoch 21 Batch 0 Loss 0.2673\n",
            "Epoch 21 Loss 0.0037\n",
            "Time taken for 1 epoch 34.69172787666321 sec\n",
            "\n",
            "Epoch 22 Batch 0 Loss 0.2420\n",
            "Epoch 22 Loss 0.0032\n",
            "Time taken for 1 epoch 33.93863844871521 sec\n",
            "\n",
            "Epoch 23 Batch 0 Loss 0.2113\n",
            "Epoch 23 Loss 0.0028\n",
            "Time taken for 1 epoch 33.60709190368652 sec\n",
            "\n",
            "Epoch 24 Batch 0 Loss 0.1598\n",
            "Epoch 24 Loss 0.0024\n",
            "Time taken for 1 epoch 40.949652671813965 sec\n",
            "\n",
            "Epoch 25 Batch 0 Loss 0.1506\n",
            "Epoch 25 Loss 0.0021\n",
            "Time taken for 1 epoch 33.401453495025635 sec\n",
            "\n",
            "Epoch 26 Batch 0 Loss 0.1363\n",
            "Epoch 26 Loss 0.0018\n",
            "Time taken for 1 epoch 33.64620780944824 sec\n",
            "\n",
            "Epoch 27 Batch 0 Loss 0.1196\n",
            "Epoch 27 Loss 0.0015\n",
            "Time taken for 1 epoch 34.43237042427063 sec\n",
            "\n",
            "Epoch 28 Batch 0 Loss 0.1005\n",
            "Epoch 28 Loss 0.0013\n",
            "Time taken for 1 epoch 34.740612268447876 sec\n",
            "\n",
            "Epoch 29 Batch 0 Loss 0.0816\n",
            "Epoch 29 Loss 0.0011\n",
            "Time taken for 1 epoch 35.45528769493103 sec\n",
            "\n",
            "Epoch 30 Batch 0 Loss 0.0648\n",
            "Epoch 30 Loss 0.0009\n",
            "Time taken for 1 epoch 35.527931213378906 sec\n",
            "\n",
            "Epoch 31 Batch 0 Loss 0.0537\n",
            "Epoch 31 Loss 0.0008\n",
            "Time taken for 1 epoch 34.9138388633728 sec\n",
            "\n",
            "Epoch 32 Batch 0 Loss 0.0521\n",
            "Epoch 32 Loss 0.0008\n",
            "Time taken for 1 epoch 36.216516733169556 sec\n",
            "\n",
            "Epoch 33 Batch 0 Loss 0.0454\n",
            "Epoch 33 Loss 0.0007\n",
            "Time taken for 1 epoch 34.28229904174805 sec\n",
            "\n",
            "Epoch 34 Batch 0 Loss 0.0494\n",
            "Epoch 34 Loss 0.0006\n",
            "Time taken for 1 epoch 33.495084285736084 sec\n",
            "\n",
            "Epoch 35 Batch 0 Loss 0.0401\n",
            "Epoch 35 Loss 0.0006\n",
            "Time taken for 1 epoch 33.33462452888489 sec\n",
            "\n",
            "Epoch 36 Batch 0 Loss 0.0347\n",
            "Epoch 36 Loss 0.0005\n",
            "Time taken for 1 epoch 33.613714933395386 sec\n",
            "\n",
            "Epoch 37 Batch 0 Loss 0.0345\n",
            "Epoch 37 Loss 0.0005\n",
            "Time taken for 1 epoch 33.466657400131226 sec\n",
            "\n",
            "Epoch 38 Batch 0 Loss 0.0249\n",
            "Epoch 38 Loss 0.0005\n",
            "Time taken for 1 epoch 34.2794873714447 sec\n",
            "\n",
            "Epoch 39 Batch 0 Loss 0.0331\n",
            "Epoch 39 Loss 0.0005\n",
            "Time taken for 1 epoch 33.43945121765137 sec\n",
            "\n",
            "Epoch 40 Batch 0 Loss 0.0237\n",
            "Epoch 40 Loss 0.0005\n",
            "Time taken for 1 epoch 33.49184775352478 sec\n",
            "\n",
            "Epoch 41 Batch 0 Loss 0.0417\n",
            "Epoch 41 Loss 0.0006\n",
            "Time taken for 1 epoch 33.33808660507202 sec\n",
            "\n",
            "Epoch 42 Batch 0 Loss 0.0308\n",
            "Epoch 42 Loss 0.0005\n",
            "Time taken for 1 epoch 33.47609734535217 sec\n",
            "\n",
            "Epoch 43 Batch 0 Loss 0.0322\n",
            "Epoch 43 Loss 0.0005\n",
            "Time taken for 1 epoch 33.577919244766235 sec\n",
            "\n",
            "Epoch 44 Batch 0 Loss 0.0319\n",
            "Epoch 44 Loss 0.0004\n",
            "Time taken for 1 epoch 33.16888117790222 sec\n",
            "\n",
            "Epoch 45 Batch 0 Loss 0.0270\n",
            "Epoch 45 Loss 0.0004\n",
            "Time taken for 1 epoch 34.23814868927002 sec\n",
            "\n",
            "Epoch 46 Batch 0 Loss 0.0225\n",
            "Epoch 46 Loss 0.0004\n",
            "Time taken for 1 epoch 33.650020360946655 sec\n",
            "\n",
            "Epoch 47 Batch 0 Loss 0.0214\n",
            "Epoch 47 Loss 0.0004\n",
            "Time taken for 1 epoch 33.63936519622803 sec\n",
            "\n",
            "Epoch 48 Batch 0 Loss 0.0268\n",
            "Epoch 48 Loss 0.0004\n",
            "Time taken for 1 epoch 33.2679705619812 sec\n",
            "\n",
            "Epoch 49 Batch 0 Loss 0.0220\n",
            "Epoch 49 Loss 0.0004\n",
            "Time taken for 1 epoch 33.19085884094238 sec\n",
            "\n",
            "Epoch 50 Batch 0 Loss 0.0218\n",
            "Epoch 50 Loss 0.0003\n",
            "Time taken for 1 epoch 33.306615591049194 sec\n",
            "\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "mU3Ce8M6I3rz"
      },
      "cell_type": "markdown",
      "source": [
        "## Translate\n",
        "\n",
        "* The evaluate function is similar to the training loop, except we don't use *teacher forcing* here. The input to the decoder at each time step is its previous predictions along with the hidden state and the encoder output.\n",
        "* Stop predicting when the model predicts the *end token*.\n",
        "* And store the *attention weights for every time step*.\n",
        "\n",
        "Note: The encoder output is calculated only once for one input."
      ]
    },
    {
      "metadata": {
        "id": "EbQpyYs13jF_"
      },
      "cell_type": "code",
      "source": [
        "targ_dict=dict(zip(range(1,len(targ_lang)+1),targ_lang))\n",
        "def evaluate(sentence, encoder, decoder, inp_lang, targ_lang, max_length_inp, max_length_targ):\n",
        "    attention_plot = np.zeros((max_length_targ, max_length_inp))\n",
        "    \n",
        "    #sentence = preprocess_sentence(sentence)\n",
        "\n",
        "    inputs = [inp_lang[i] for i in sentence.split(' ')]\n",
        "    print(inputs)\n",
        "    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs], maxlen=max_length_inp, padding='post')\n",
        "    inputs = tf.convert_to_tensor(inputs)\n",
        "    \n",
        "    result = ''\n",
        "\n",
        "    hidden = [tf.zeros((1, units)),tf.zeros((1, units))]\n",
        "    enc_out, enc_hidden = encoder(inputs, hidden)\n",
        "\n",
        "    dec_hidden = enc_hidden\n",
        "    dec_input = tf.expand_dims([2], 0)\n",
        "\n",
        "    for t in range(38):\n",
        "        predictions, dec_hidden, attention_weights = decoder(dec_input, dec_hidden, enc_out)\n",
        "        \n",
        "        # storing the attention weigths to plot later on\n",
        "        attention_weights = tf.reshape(attention_weights, (-1, ))\n",
        "        attention_plot[t] = attention_weights.numpy()\n",
        "\n",
        "        predicted_id = tf.compat.v1.multinomial(tf.exp(predictions), num_samples=1)[0][0].numpy()\n",
        "\n",
        "        result += targ_dict[predicted_id]+'_'+str(predicted_id) + ' '\n",
        "\n",
        "        if targ_dict[predicted_id] == '<end>':\n",
        "            return result, sentence, attention_plot\n",
        "        \n",
        "        # the predicted ID is fed back into the model\n",
        "        dec_input = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "    return result, sentence, attention_plot"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "targ_dict[targ_lang['quen']]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "3HZcMopkijUz",
        "outputId": "d035c8bb-7e94-4118-c7ed-73287fa543f5"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'quen'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "metadata": {
        "id": "s5hQWlbN3jGF"
      },
      "cell_type": "code",
      "source": [
        "# function for plotting the attention weights\n",
        "def plot_attention(attention, sentence, predicted_sentence):\n",
        "    fig = plt.figure(figsize=(10,10))\n",
        "    ax = fig.add_subplot(1, 1, 1)\n",
        "    ax.matshow(attention, cmap='viridis')\n",
        "    \n",
        "    fontdict = {'fontsize': 14}\n",
        "    \n",
        "    ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
        "    ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
        "\n",
        "    plt.show()"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sl9zUHzg3jGI"
      },
      "cell_type": "code",
      "source": [
        "def translate(sentence, encoder, decoder, inp_lang, targ_lang, max_length_inp, max_length_targ):\n",
        "    result, sentence, attention_plot = evaluate(sentence, encoder, decoder, inp_lang, targ_lang, max_length_inp, max_length_targ)\n",
        "        \n",
        "    print('Input: {}'.format(sentence))\n",
        "    print('Predicted translation: {}'.format(result))\n",
        "    \n",
        "    attention_plot = attention_plot[:len(result.split(' ')), :len(sentence.split(' '))]\n",
        "    plot_attention(attention_plot, sentence.split(' '), result.split(' '))"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WrAM0FDomq3E",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 742
        },
        "outputId": "e2556f35-8de7-45df-fde6-61256488104e"
      },
      "cell_type": "code",
      "source": [
        "translate('start '+'i love rain a lots'+' end', encoder, decoder, inp_lang, targ_lang, max_length_inp, max_length_targ)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/util/dispatch.py:1082: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.random.categorical` instead.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2, 5, 214, 210, 8, 1505, 1]\n",
            "Input: start i love rain a lots end\n",
            "Predicted translation: tôi_4 muốn_26 xem_147 ti-vi_1124 cả_86 ngày_99 ._3 <end>_2 \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x720 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgAAAAJZCAYAAADf+Y3FAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deZwlVX338c9XZhgEBUVQwA2XqKBxnYj7Avrgjo9xQ1yQRDRqokYf9wXjvktUVIyCRiAqMbiL4oJLQBmQyCaIyiYCIiAMyzDA7/mjauRyM0v3THfX7T6f9+vVr+5bdarqV00z91unTp2bqkKSJLXlRkMXIEmS5p4BQJKkBhkAJElqkAFAkqQGGQAkSWqQAUCSpAYZACRJapABQJKkBhkA5qEkz02yZDXLN07y3CFqkiTNL3EmwPknybXAtlV1wdjyWwAXVNVGw1QmSZov7AGYnwKsLrndDvjzHNciSZqHFg1dgKYuyQl0b/wFHJnkmpHVGwG3B745RG2SpPnFADC/HNp/vwfwDWD5yLqrgTOA/5zjmiRJ85BjAOaZJIuAFwKHVdXvh65HsyfJrYDnAHcC3lRVFyZ5MHBuVf1u2OokzXeOAZhnquoa4APA4qFr0exJcj/gVGAP4O+AzftVjwbeMVRdkhYObwHMT/8D3Jmuy18L0/uBfavqLUkuG1l+OPD8gWqaM0luxtgFSlVdNFA50pQkud1U21bVWbNZy1QYAOanfYAPJHkLcCxw+ehK/6FcEO5Hd+U/7g/Area4ljmR5PbAJ4BHABuPrqIb+OrjrZp0Z7D6J7RWZ/C/ZwPA/PSN/vuXueEfm/9QLhxXAjdfzfK7AResZvlCcABwM7rgcy5T/4dU80SSHYFrq+rU/vWjgecBJwHvraprh6xvBvzNyM93Ad5LF2qP6pc9kG4M12vmuK7VchDgPJTk4WtbX1VHzlUtmh1J9ge2AZ4GXAjck+4N8SvA96vqFQOWNyuSLAceUFUnDl2LZkeSo4EPV9V/JLkt3TiXH9L9ff97Vb1uyPpmUpIjgY9U1aFjy58KvKyqHjpMZSO1GACkyZNkc7o5He4JbAacR9f1/1PgcVV1+Vo2n5f6eS72rKpjh65FsyPJJcD9q+q0JK8AnlRVj0zySOCAqtp+2ApnTpIrgXtV1Wljy+8CHF9Vmw5T2fW8BTCPJdmObva/0fulVNWPhqlIM6WqLgUekmRn4L50A+KOq6ojhq1sVr0MeFeSF1fV6UMXo1mxEd2cJQC7cP3EZb9h4Y1tOQN4MfDyseUvBs6c82pWwx6Aeah/4z8YeBhdt/ANpgb2swDmvyT3rqrjh65jLvVPOyyhe5NYAYzOdElVbb667TR/JDkK+BHwdeA7dL0BJyR5IPDFqrrtoAXOoCSPAf6L7s3+6H7xTsD2wFOq6lsDlfYX9gDMTx8GrgV2BI4BHkOXnv8FWHD3hht1XJKTgX8HDqqqc4YuaA68dOgCNOteAxwGvAr4bFWd0C9/EvDzwaqaBVX17SR/RXfFf7d+8ZeBT1TV2cNVdj17AOahJOcDj6+qZUkuBZb299QeTzdj3AMGLlEbqL9PuAewO3BH4Cd0YeDQqvIDnzRvJdkI2LyqLh5Ztj1weVX9cai6WmQAmIf6N/17VtUZSc4Anl1VP0lyB+CkSRhcopmTZCe6MPB0uhkBv1FVTxu2qpmRZMtV81Yk2XJtbZ3fYv5L8hm6EfCXjS3fjG7E/F7DVDY7kmwK3Bu4Jf97YqsvD1LUCAPAPJTk58Cb+y6mw+g+FOgNwD8Cu1XVXw1aoGZFHwQ+QRf+FsQ4jyTXAttW1QVJrmP1z/4HqIVyzi0b/e89tnwr4LyqWjC3pZM8CjgEuMVqVk/E3/OC+WU3Zl+6Z8Shu+//bbqu4hV0k2pogeh7dfbov+5MN4Dq7wctambtDKy6sn/kkIVo9vS9O+m/br6ajzJ/PHD+ELXNon3pJm17fVWdO3Qxq2MPwALQdzPdDTirqi4cuh5tuCQvoXvT3wk4Efg8cLCfAKn5aC29O6sU8JaqWjAfdJXkcrreut8MXcuaGADmoSRvBt5fVVeMLb8x8P+q6l+GqUwzJclZdN2Hnx8ZKd2MJNvwv+e3GPzDU7R++tlLA3wf+Fuu7/WBbl6AMyf1Knl9JfkO3ayH31xn44EYAOahtdxHuwVwwSTcW9KGSZJq7H/OJFsA/0o32HHj8fX+Xc9//Qc+nV1V1w1dy2xL8hTg7cAHgROAlaPrq+q4Ieoa5RiA+ekGE/+MuA83TNaap6qqktwKeAndfA8FnAzsV1UL7V7pKu8H7gU8me556b2AW9PNEPjKAevSDKmqM5Pcqr/Fterv+iTg4wvw73rVZwDsv5p1E/GhbfYAzCP9TGlFNzf8FdwwBGwEbEI3ycRLBihPMyjJg+kGd57PDT9J7JbArlV11Jq2na+SnAPsXlU/7h91vW9VnZ5kd2Cvqnr0wCVqA7X0d933dqxRVQ0+HbABYB5J8jy6q//P0M0vPTohzNXAGQvpf6CW9VOmngC8aFV3aZIb0T0GeI+qetCQ9c2G/tMAd6yqs5KcDTy1qn7WTxJzUlVtNmiBsyDJIuD+rP4zPT43SFGzqLW/6ySPpevFuyNdwDk7yd8Dv6uq7w1bnbcA5pWq+iz8ZdKMH60aHDb6mdpJfr4APlNb3eQhe47eK62q65J8EPjFcGXNqt/Q/UN5FnAK8Mx+zounsABvbSW5G/A14A50wf5aun+TV9I90rvgAgAN/V0n2YMu2Pwb3QcfLe5XbQS8Ghg8ANxo3U00gZ4D3B2g/0ztw4At6ZLm2wesSzPnz3RvDOPuAFwyx7XMlQPpPv4Y4N3AC+l6tt4HvGegmmbTh4FjgS3obuntACwFjqcbKb8QtfR3/WrgBVX1Cm74wVZH0wWhwdkDMD/dDVg1gvSpwM+r6nGrPlMbeN1glWmm/Afw6SSvBv67X/ZgujfCQwarapYkWQw8E3guQFV9v79CXgr8eoE+Cvk3wMOr6vL+OflFVXVc/9/8I1wfhhaSlv6u/4rrxzmMWk43pffgDADzU0ufqd2qV3P9eI9F/c9XAx8HXjtgXbOiqlb2sx6Ofqz1WXS3Axaq0F35A/yR7omHU4Fz6GZ9XIha+rs+F7gL3ccBj3oY3b/Vg/MWwPx0IvAPSR5KFwC+3S+/NeBMgAtAVV1dVS8Dbk7XXXgvYMuqekVVXb32reetzwIvGLqIOXQi3X9X6D4K9zX9hDlvBU4frKpZ1Njf9f7Av/ZPPgDcth/I/V66wDM4ewDmpyY+UzvJV+k+6fDS/uc1qqonzVFZs2Zd59i3ARbG+a7GZsAe/aDWY4HLR1dW1T8NUtXseQfdOQO8kW7e+B/QhfinD1XUTGv177qq3ttPbvVduke0f0A3uPP9VfWxQYvrGQDmoar6UZKtGftMbeCTXN+luBD8ieu7hP80ZCFzpIVzXJsduH5syx3H1i2455Wr6vCRn38L7NB/aM7FC2wWyGb/rqvqDUneQTfp0Y2Ak6tq+cBl/YXzAEiS1CDHAEiS1CADgCRJDTIALABJ9h66hrnW2jl7vgtfa+fc2vnC5J2zAWBhmKg/qjnS2jl7vgtfa+fc2vnChJ2zAUCSpAb5FMAM2jhLahPm/gPLVrKCxSyZ8+MOqbVzHup8r73FMB/Ad81Vl7Nok2GOvdEthpmPZuWfr2TxFjee+wOftnLuj0l7/w/DcOd8GRdfWFVbjy93HoAZtAmbsVN2GboMacZc8oQHDl3CnLvZ884euoS5tcs5Q1egWXZEHTo+HTHgLQBJkppkAJAkqUEGAEmSGmQAkCSpQQYASZIaZACQJKlBBgBJkhpkAJAkqUEGAEmSGmQAkCSpQQYASZIaZACQJKlBBgBJkhpkAJAkqUEGAEmSGmQAkCSpQQYASZIaZACQJKlBBgBJkhpkAJAkqUEGAEmSGmQAkCSpQQsiAKTzqiQ7DV2LJEnzwYIIAMCLgccCBya5yeiKJAcm+fowZUmSNJnmVQBI8sMkHx1bdgfg74AnAx8E3je22cuAZ6/HsV6XpMaPJ0nSQrBo6AI2VFX9Drhv//JTq1n/5+nuM8kDgL2BX25YdZIkTaZ50wOQ5EDg4cBL+ivzSrJ9kocl+VmSq5Kcn+RDSTYe3W46twCSbAEcBOwFXDzT5yFJ0iSYNwGAriv/KOAAYNv+ayXwLeAXwH3obgXsDrxrA46zP3BoVf1gg6qVJGmCzZsA0HflXw1cUVXnVdV5dIP/zgVeXFWnVNXXgdcCL02y6XSPkeQFwJ2BN05jm72TLEuybCUrpntISZIGMW8CwBrsABxdVdeNLPsJsDHdG/mUJbkr8E7gWVW1cqrbVdX+VbW0qpYuZsl0DilJ0mDm/SDAtahptn8gsBVwUpJVyzYCHpbkRcBmVeUlviRpQZhvAeBqujflVU4Bnp7kRiO9AA/p2/1mmvs+DFg2tuwA4Nd0PQNXT79cSZIm03wLAGcA90+yPbAc2A94ObBfkn2BOwLvBj5aVVdMZ8dVdQlwyeiyJJcDF1XViRtcuSRJE2S+jQF4P92V+MnAH4HFdDMA3gc4HvgMcAjw+qEKlCRpPphXPQBVdRrdvfpRZwBr/AyAqtpzA473iPXdVpKkSTbfegAkSdIMmFc9ABsiye3obh2syY5VddZc1SNJ0pCaCQB0Ewbdex3rJUlqQjMBoKquAU4fug5JkiaBYwAkSWqQAUCSpAYZACRJapABQJKkBhkAJElqkAFAkqQGGQAkSWqQAUCSpAYZACRJapABQJKkBhkAJElqkAFAkqQGGQAkSWqQAUCSpAYZACRJapABQJKkBhkAJElqkAFAkqQGLRq6AEmT689PWD50CXPuym/ebugS5tS2nDN0CRqIPQCSJDXIACBJUoMMAJIkNcgAIElSgwwAkiQ1yAAgSVKDDACSJDXIACBJUoMMAJIkNcgAIElSgwwAkiQ1yAAgSVKDDACSJDXIACBJUoMMAJIkNcgAIElSgwwAkiQ1yAAgSVKDDACSJDXIACBJUoMMAJIkNcgAIElSgwwAkiQ1yAAgSVKDJjoAJHlgkr8fug5JkhaaiQ0ASW4GHAScOHQtkiQtNBMVAJLcJcnvk/wP8NfAS6vq6KHrkiRpoZmYAJBkI+DTwLOALwJPqqpvzuD+X5fkmCSXJvljkq8lucdq2t0lyZeTXJLkiiTHJdlhpuqQJGkSLBq6gBEBdquqi4Ajk9xyhvf/CGA/4Jj+WP8CHJFkx/6YJLkD8FPgc8DOwCXA3YDlM1yLJEmDmnYPQJIfJvl4kg8kuai/mn5ZkiVJPtZfOZ+V5Dl9++2TVJKlY/upJE8dWbQD8MUkVya5CHhvki1G2h+Y5Ov9sX6f5OIkByTZdCp1V9WuVXVAVZ1YVScAzwG2Bh480uwdwHeq6pVVdVxV/baqvllVZ0/39yRJ0iRb31sAewCXATsB7wY+DBwGnAYsBT4L/FuSbaeysySbAYfTXWnfH/i/wIOAz4w1fShwD+BRwDP6di9bz3O4Kd35X9zXcCPgicDJSb7dB5tjkjxjHbXvnWRZkmUrWbGepUiSNLfWNwCcVFX7VNWvgQ8CFwIrq2rfqjqdrns93PDqem2eBWwGPKeqTqiqI4G9gackufNIu0uBF1XVKVX1HeBLwC7reQ77AscDR/WvbwncBHg98B3g0cAhwEFJHr+mnVTV/lW1tKqWLmbJepYiSdLcWt8xAL9c9UNVVZILgBNGlq1McjHdm+pU7AD8sqouG1n238B1wI7A6f2yk6vq2pE259L1QkxLkg8CDwEeMrK/VWHoK1X1wf7n4/tbFy8FvjHd40iSNKnWtwdg5djrWsOyG9G9iUPXI9D9kCyexrFqHced1jkk+RCwO7BzVf12ZNWFwDXAyWObnALcbjrHkCRp0s3FY4B/7L+Pjge491ibU4C/TnLTkWUPoqvvlJkqJMm+XP/m/6vRdVV1Nd0TAncd2+wuwJkzVYMkSZNg1h8DrKorkxwNvCbJb4AtgHeNNTsIeCvwuSRvBm4OfBL4cj+mYIMl+RjdyP8nAxcn2aZftbyqVj3m9166JxF+DHwfeCTwzH4bSZIWjLmaCGiv/vsxdG/sbxxdWVVXALsCmwM/B75CNzhvL2bOi+lG/n8P+MPI16tG6jiMbvDhq+jGNPwj8Nyq8v6/JGlBSVWtu5WmZPNsWTtlfR9KkCbPmV/866FLmHMb/+ym6260gGz7gf8eugTNsiPq0GOraun48omZCliSJM2dSZoKeL0luR3/e/T+qB2r6qy5qkeSpEm3IAIA3XwA408WjK+XJEm9BREAquoarp8sSJIkrYNjACRJapABQJKkBhkAJElqkAFAkqQGGQAkSWqQAUCSpAYZACRJapABQJKkBhkAJElqkAFAkqQGGQAkSWqQAUCSpAYZACRJapABQJKkBhkAJElqkAFAkqQGGQAkSWqQAUCSpAYtGroASZPr9k8/YegSNMsOP/f4oUuYc7tud++hS5gI9gBIktQgA4AkSQ0yAEiS1CADgCRJDTIASJLUIAOAJEkNMgBIktQgA4AkSQ0yAEiS1CADgCRJDTIASJLUIAOAJEkNMgBIktQgA4AkSQ0yAEiS1CADgCRJDTIASJLUIAOAJEkNMgBIktQgA4AkSQ0yAEiS1CADgCRJDTIASJLUIAOAJEkNMgBIktQgA4AkSQ1aEAEgyd2THJrkt0kqyT7raP+6vt1Hx5bXGr4+NqsnIEnSHFsQAQDYFDgDeCPwu7U1TPIAYG/gl6tZve3Y1xP75V+cqUIlSZoE0w4ASbZO8ockbxlZds8kVyV5WpKNk7wnyTlJrkhyTJJdR9o+or+qfmySY5NcmeTHSW6T5OFJ/ifJ8iRfT3KLqdRUVcdU1auq6mDgirXUvgVwELAXcPFq9nPe6BewG3BaVR059d+QJEmTb9oBoKr+COwJvCHJA5PcGDgEOKSqvgQcADwceBZwD+CzwNeS3GtsV28FXg7sBNwc+ALwZrqr80cAdwf2mfYZrd3+wKFV9YN1NUxyE+CZwKdmuAZJkga3aH02qqrDk+xHdzV9JLAE+MckdwJ2B7avqrP65h9N8ijghcCLR3bzpqr6MUCSTwAfAe5XVcf1yz4LPHV96ludJC8A7gw8e4qbPAvYmC7ArG2/e9OFFjZh0w0pUZKkObNeAaD3GuAxwHOBB1XV8iSPBQKcnGS07RLg+2Pbj96DP7//fsLYsltuQH1/keSuwDuBh1TVyilu9gLgK32PxxpV1f50PQtsni1rgwqVJGmObEgA2B64LVDAHYGf0d1SKOBvgPE32ivHXo+uL4CxN+di5gYpPhDYCjhpJJhsBDwsyYuAzapqxaoVSe4NLAVeP0PHlyRpoqxXAEiyGDgY+CrdG/9+SX4K/IKuB2Cbqdxnn0OHAcvGlh0A/JquZ+DqsXV70z1NcMTslyZJ0txb3x6AtwFbA7sAf6a7FfA5YGe6cQEHJnklcBywJd2gvt9W1Zc3tODVSbIxsGP/chNgm/4qfnlVnV5VlwCXjG1zOXBRVZ04tnxTYA/gvVVll74kaUGadgBI8nDglcCj+zdWkuxJd0//NcDzgTcA7wVuA1wE/ByYzR6B7eh6H1a5E92gwyPpwsd0PAPYjK6HQJKkBSle5M6czbNl7ZRdhi5Dkqbs8HOPH7qEObfrdvceuoQ5dUQdemxVLR1fvlBmApQkSdOwIU8BzJkky9ey+rGr5hOQJElTMy8CALC2/prfz1kVkiQtEPMiAFTV6UPXIEnSQuIYAEmSGmQAkCSpQQYASZIaZACQJKlBBgBJkhpkAJAkqUEGAEmSGmQAkCSpQQYASZIaZACQJKlBBgBJkhpkAJAkqUEGAEmSGmQAkCSpQQYASZIaZACQJKlBBgBJkhpkAJAkqUGLhi5AkjScxz/oSUOXMOe2O/rioUuYWzutfrE9AJIkNcgAIElSgwwAkiQ1yAAgSVKDDACSJDXIACBJUoMMAJIkNcgAIElSgwwAkiQ1yAAgSVKDDACSJDXIACBJUoMMAJIkNcgAIElSgwwAkiQ1yAAgSVKDDACSJDXIACBJUoMMAJIkNcgAIElSgwwAkiQ1yAAgSVKDDACSJDXIACBJUoMMAJIkNWi9AkCS7ZNUkqXre+AkeyZZvr7bS5Kk9TelAJDkh0k+OrLobGBb4PgNOPYXgDtO8fjbJjk4ya+SXJvkwNW0uXuSQ5P8tg8n+6ymzeuSHJPk0iR/TPK1JPdYy3E/2e/rVdM4L0mSJt569QBU1bVVdV5VXbO+B66qK6vqgik2XwJcCLwb+Nka2mwKnAG8EfjdGto8AtgPeBCwM3ANcESSLccbJnkqcH/g3CnWKEnSvLHOANBfbT8ceEl/NVzrugWQZO8k5yfZaGz5wUm+2v885VsAVXVGVf1TVR0IXLSGNsdU1auq6mDgijW02bWqDqiqE6vqBOA5wNbAg8fqvD2wL/AsYOVUapQkaT6ZSg/Ay4CjgAPouv23BTZa6xbwJWAL4NGrFiS5CbAb8Pn1qnR23JTud3DxqgVJFgGHAG+vqlOGKkySpNm0zgBQVX8Grgau6Lv9zwOuXcc2FwPfBPYYWfxkui73r65/uTNuX7pxDEeNLHsrcGFVfXwqO+h7O5YlWbaSFbNRoyRJM25GHgNMclKS5f3Xt/rFnweenGTT/vUewH9W1VUzccwNleSDwEOAv62qa/tljwD2BP5uqvupqv2ramlVLV3MktkoVZKkGbdohvbzOGBx//OV/fdv0F3x75bke8CjgF1n6HgbJMmHgGcCj6yq346segTdLY4/JFm1bCPgPUleXlW3mdNCJUmaJVMNAFezlvv+VXXmapatSPIluiv/rYDzgB+uR40zKsm+wDPo3vx/NbZ6P+DQsWWH040J+NQclCdJ0pyYagA4A7h/ku2B5Uz91sHnge8BdwAOqarrplnfXyS5d//j5sB1/eurq+rkfv3GwI59m02Abfo2y6vq9L7Nx+hG/j8ZuDjJNn375VW1vH8s8QaPJiZZCZxXVaeub+2SJE2aqQaA9wOfBU4Gbkz3hj4VPwZ+T/fGvPu0q7uhX4y9fiJwJrB9/3q7sTZ3Al4IHEnXtQ/w4v7798b29VZgnw2sT5KkeWNKAaCqTgMeOLY4q2s7tl1x/Rv0+LoDgQOncvy+/VqPV1VnrKumde1jDdtsP91tJEmadH4YkCRJDZqIADD2GOH41x7r3oMkSZqOmXoMcEONPkY47vy5LESSpBZMRABY3WOEkiRp9kzELQBJkjS3DACSJDXIACBJUoMMAJIkNcgAIElSgwwAkiQ1yAAgSVKDDACSJDXIACBJUoMMAJIkNcgAIElSgwwAkiQ1yAAgSVKDDACSJDXIACBJUoMMAJIkNcgAIElSgwwAkiQ1yAAgSVKDFg1dgCRpONeccdbQJcy537zr/kOXMBHsAZAkqUEGAEmSGmQAkCSpQQYASZIaZACQJKlBBgBJkhpkAJAkqUEGAEmSGmQAkCSpQQYASZIaZACQJKlBBgBJkhpkAJAkqUEGAEmSGmQAkCSpQQYASZIaZACQJKlBBgBJkhpkAJAkqUEGAEmSGmQAkCSpQQYASZIaZACQJKlB8zIAJHl8ktOT/CLJ7YauR5Kk+WZeBgDgT8D/BY4H7jFwLZIkzTvzMgBU1dHAdsCfquqb09k2ybOSHJ/kiiTnJfl8km3G2mye5F+TnJtkRd/b8PSZPAdJkoa0aOgCNsDJwE+ms0GSBwP/DrwKOAy4FbAfcBCwS99mMfBd4CLg6cA5wG2AFTNVuCRJQ5vIHoB0Xpnk1/0V+DlJ3tWve3eSU4HTgJOSvDfJJlPc9QOBc6rqQ1X1u74n4SPATiNtng9sDexWVT+pqjP678fM4ClKkjSoiQwAwDuBNwHvAu4OPA04u193ObAXsAPwYuCZwBumuN+fAtsmeWIfMrbqtx+9jfDkvt1H+lsEJyfZp+8ZkCRpQZi4WwBJbgK8Anh5VX2mX3w6cBRAVb1tpPkZSd5J16X/pnXtu6qOSvJMui7/G9Od/3eB5400uyOwM3Aw8Hhge+BjwE3640iSNO9NYg/AjsAS4HurW5nkqUl+0l+dLwc+BEzpUcAkO9J1+b8NuB/wGGAb4JMjzW4EXAC8oKqOrar/BN4M/EOSrGafeydZlmTZSocJSJLmiUkMAGuU5AHAfwCHA08E7gO8EZhq9/zrgJ9X1fuq6pdVdTjdbYTnJLlN3+YPwGlVde3IdqcAmwJbje+wqvavqqVVtXQxS9brvCRJmmuTGABOoRtxv8tq1j0Y+H1Vva2qjqmqXwO3n8a+NwWuHVu26vWq38VPgTsnGf3d3AW4ArhwGseSJGliTdwYgKq6LMm+wLuSrAB+BNyCrsv+NODWSfagGxOwK7D7NHb/NeBTSf6BrhdhW+DDwHFVdVbf5uPAS4F9k3yUbgzAW4H9qqo29PwkSZoEExcAeq8DLqYb2Hcb4Hzgc1X18STvo3vTvjHwHbr78/tNZadVdWCSm9K9wX8A+DPwfeA1I23OTvJ/gA/SzTR4HvAZ4O0zc2qSJA0vXtTOnM2zZe2U1d25kCRNiit3u//QJcypnx726mOraun48kkcAyBJkmbZggoASU5KsnwNX3sMXZ8kSZNiUscArK/HseZHAs+fy0IkSZpkCyoAVNWZQ9cgSdJ8sKBuAUiSpKkxAEiS1CADgCRJDTIASJLUIAOAJEkNMgBIktQgA4AkSQ0yAEiS1CADgCRJDTIASJLUIAOAJEkNMgBIktQgA4AkSQ0yAEiS1CADgCRJDTIASJLUIAOAJEkNMgBIktQgA4AkSQ1aNHQBkiTNpU2//T9DlzAR7AGQJKlBBgBJkhpkAJAkqUEGAEmSGmQAkCSpQQYASZIaZACQJKlBBgBJkhpkAJAkqUEGAEmSGmQAkCSpQQYASZIaZACQJKlBBgBJkhpkAJAkqUEGAEmSGmQAkCSpQQYASZIaZACQJKlBBgBJkhpkAJAkqUEGAEmSGmQAkCSpQQYASZIaNFEBIMltkrwpyWZD1yJJ0kI2UQGgqs4BbgF8cuhaJElayCYqAPT+GbhxkhfO9I6T3DfJd5NckuRPSfZPcpOxNrsk+e8klyU5L8l7km0V6+cAAA1bSURBVCya6VokSRrSxAWAqrquqv62qma0FyDJdsARwG+BnYDHAHcHDhxpcy/gm8B3gfsAzwCeBLx7JmuRJGloMxYAkvwwyX5J3pnkwiQXJHl/khv162+V5KtJrkxyZpLnJzkxyT4j+/jnJL9McnmS3yf5tyQ369dtluTSJE8dO+6jk6xMcqt1lPgE4DrgxVV1alUdA7wI+Nskd+7bPAM4uareUlWnV9WRwKuBlyS56Uz8niRJmgQz3QOwB3AN8CDgpcDL6d5UAT4L3B7YGdgNeHb/etR1/TZ3B54F3B/4CEBVXQ4cAuw1ts1ewNer6vx11LYEWFlV144su7L//pCRNleNbXclsAlwv3XsX5KkeWOmA8DJVfXmqjqtqr4I/ADYJcldgV2BF1bVUVV1PLAnsOnoxlX14ar6flWdMXL1/fRVvQjAp4D/k+TWAEluDjwZ+PQUavs+sFWS1ybZuN92Vdf+tv33w4Gdkjw7yaL+OG8ea3MDSfZOsizJspWsmEIZkiQNb6YDwC/HXp8L3BK4G93V/bJVK6rq7H79XyTZuR+kd06Sy4AvAxsD2/TbLANOAJ7Xb/Is4CLgW+sqrKpO6rd7Od1V/XnA74Dz+9qoqu8ArwI+RtcTcBrdmABWtVnNfvevqqVVtXQxS9ZVhiRJE2GmA8DKsdc11WMkuT3wDeAU4Gl0Xe6ruvs3Hmn6b3S9B/TrPzvWrb9GVXVwVW0DbEf3uOE+wNZ0AwNXtfkgcDPgdsBWwFf6Vb9FkqQFYq6eAvhVf6y/3EdPchu6N+JVltK90b+iv01w2tj6VQ4CbpPkpcB9gQOmW0xVnV9Vy+nGJ1xFN+p/dH1V1blVdSWwO3A2cNx0jyNJ0qSak+fbq+rUJIcDn0jyD3Rvuu8DrqDrJQD4NV1IeHmSLwMPoOuuH9/XJUm+BHwA+FFV/XqqdfSh4SjgMuDRfQ2vrapLRtr8P+DbdF3+TwFeCzx9qr0MkiTNB3M5D8CewDnAD4Gv0l3JX0A/6r6qfgm8jG4ioJOBv6e7H786n6brLZjK4L9R9we+QzeOYG+6QYn/OtbmscCP6cYrPB7YraoOm+ZxJEmaaKmqdbeajQMnW9ENAty9qv5zmts+g2664O2q6orZqG99bJ4ta6fsMnQZkqS1yJK2Bmx/96qDjq2qpePL52yK2yQ7Azelu/q+JfAO4EK67vap7mNTuicCXg98apLe/CVJmk/m8hbAYuDtdAHga3T3/x/WT/AzVa8GTqV79O9toyuSvD7J8jV8rfMxQUmSWjLYLYCZlmRLYMs1rL6yqn4/2zV4C0CSJp+3ADoL5lPuquoiup4BSZK0DhP3aYCSJGn2GQAkSWqQAUCSpAYZACRJapABQJKkBhkAJElqkAFAkqQGGQAkSWqQAUCSpAYZACRJapABQJKkBhkAJElqkAFAkqQGGQAkSWqQAUCSpAYZACRJapABQJKkBhkAJElqkAFAkqQGLRq6AEmS5lKtWDF0CRPBHgBJkhpkAJAkqUEGAEmSGmQAkCSpQQYASZIaZACQJKlBBgBJkhpkAJAkqUEGAEmSGmQAkCSpQQYASZIaZACQJKlBBgBJkhpkAJAkqUEGAEmSGmQAkCSpQQYASZIaZACQJKlBBgBJkhpkAJAkqUEGAEmSGmQAkCSpQQYASZIaZACQJKlBBgBJkhpkABiRZOskhyc5N8mKJGcn+ViSLYauTZKkmWQAuKHrgP8CngjcBdgT2AX41IA1SZI04xYNXcAkqao/AZ8YWXRmkv2A1w1UkiRJs8IegLVIsh3wFODIoWuRJGkmGQBWI8khSa4Afg9cBjx/LW33TrIsybKVrJizGiVJ2hAGgNV7BXBfYDfgjsCH19SwqvavqqVVtXQxS+aqPkmSNohjAFajqs4DzgN+leQi4MdJ3l5VZw9cmiRJM8IegHVb9Tvy8l6StGA00QOQ5KXAS6vqbuto9wTgFsCxwHLg7sD7gKOr6vRZL1SSpDnSRAAAtgLuOoV2VwEvAnagu+I/m25egHfPXmmSJM29JgJAVe0D7DOFdkcAR8x2PZIkDc0xAJIkNaipAJDkE0mWr+HrE+vegyRJC0MTtwBGvBl4/xrWXTqXhUiSNKSmAkBVXQBcMHQdkiQNralbAJIkqWMAkCSpQQYASZIaZACQJKlBBgBJkhpkAJAkqUEGAEmSGmQAkCSpQQYASZIaZACQJKlBBgBJkhpkAJAkqUEGAEmSGmQAkCSpQQYASZIaZACQJKlBBgBJkhpkAJAkqUEGAEmSGrRo6AIkSZpLWbJk6BLm1lWrX2wPgCRJDTIASJLUIAOAJEkNMgBIktQgA4AkSQ0yAEiS1CADgCRJDTIASJLUIAOAJEkNMgBIktQgA4AkSQ0yAEiS1CADgCRJDTIASJLUIAOAJEkNMgBIktQgA4AkSQ0yAEiS1CADgCRJDTIASJLUIAOAJEkNMgBIktQgA4AkSQ0yAEiS1CADgCRJDZqXASDJq5KcMXQdkiTNV/MyAMyWJIuTvCfJL5NcnuQPSQ5Ocruha5MkaSbNeABIsnmSm830ftdxzK2TbDIDu9oUuC/wjv77bsBtgW8nWTQD+5ckaSLMSABIslGSXZMcDJwH3KtfvkWS/ZNckOSyJEcmWTqy3Z5JlifZJcmJ/VX3D5LcYWz/r05yXt/2c8BNxkp4HHBef6wHr+95VNWfq+rRVfWFqjq1qn4OvBDYof+SJGlB2KAAkOTuSd4LnA18AbgceAzwoyQBvgHcGngCcB/gR8D3k2w7spslwOuAvYAHAjcDPjFyjKcDbwfeQndVfirwz2OlHAQ8C7gp8N0kpyd583iQWE+b998vnoF9SZI0EaYdAJLcIsk/JTkW+AVwN+BlwDZV9YKq+lFVFfBI4N7AU6vq51V1elW9Cfgt8JyRXS4CXtK3+SXwfuARfYAAeDnw2ar6ZFWdVlXvAH4+WlNVXVNV36yq3YFtgHf2x/91kh8m2SvJeK/BVM51Y+ADwNeq6pw1tNk7ybIky1ayYrqHkCRpEOvTA/CPwL7AVcBdqupJVfWlqrpqrN396O6p/7Hvul+eZDlwD+BOI+1WVNWpI6/PBTYGbt6/3gE4amzf46//oqourarPVNUjgb8BbgV8GnjqdE6yv+f/eboeieev5Xj7V9XSqlq6mCXTOYQkSYNZn4Ft+wMrgecCJyb5L+Dfge9V1bUj7W4EnA88dDX7uHTk52vG1tXI9tOWZAndLYdn040NOImuF+Er09jHIuAQ4K+BR1TVn9anFkmSJtW032Sr6tyqekdV3RV4FLAc+A/gnCQfSHLvvulxdFff1/Xd/6NfF0zjkKcADxhbdoPX6TwkySfpBiF+BDgduF9V3beq9q2qKd3DT7KYbjzDPYFHVtV506hVkqR5YYMebauqo4Gjk7wceCLwPOCYJDsDRwA/Bb6S5NXAr+juzz8GOKKqfjzFw+wLfC7JMcAP6brydwIuGmnzbOCTdFf5uwPfHeuNmJL+yv9LdLcOnghUkm361X+uqiunu09JkibRjDzbXlUrgEOBQ5PcEri2qirJ4+hG8H8KuCXdLYGfAp+bxr6/kOSOdM/mbwp8FfggsOdIs+/RDUK89H/vYVpuQ/fsP8CxY+ueDxy4gfuXJGkipBuwr5mwebasnbLL0GVIktYiS9oasP3dqw46tqqWji93KmBJkhrU1PS2SR4KfGtN66tq2nMFSJI0HzUVAIBldJMTSZLUtKYCQD+K//Sh65AkaWiOAZAkqUEGAEmSGmQAkCSpQQYASZIaZACQJKlBBgBJkhpkAJAkqUEGAEmSGmQAkCSpQQYASZIaZACQJKlBBgBJkhpkAJAkqUEGAEmSGmQAkCSpQQYASZIaZACQJKlBBgBJkhq0aOgCJEmaS7VixdAlTAR7ACRJapABQJKkBhkAJElqkAFAkqQGGQAkSWqQAUCSpAYZACRJapABQJKkBhkAJElqkAFAkqQGGQAkSWqQAUCSpAYZACRJapABQJKkBhkAJElqkAFAkqQGGQAkSWqQAUCSpAYZACRJapABQJKkBhkAJElqkAFAkqQGGQAkSWqQAUCSpAYZACRJapABQJKkBhkAJElqkAFAkqQGGQAkSWqQAUCSpAYtGrqA+S7J3sDeAJuw6cDVSJI0NfYAbKCq2r+qllbV0sUsGbocSZKmxAAgSVKDDACSJDXIACBJUoMMAJIkNcgAIElSgwwAkiQ1yAAgSVKDDACSJDXIACBJUoMMAJIkNcgAIElSgwwAkiQ1yAAgSVKDDACSJDXIACBJUoMMAJIkNcgAIElSgwwAkiQ1yAAgSVKDDACSJDXIACBJUoMMAJIkNcgAIElSgwwAkiQ1yAAgSVKDDACSJDXIACBJUoMMAJIkNShVNXQNC0aSPwJnDnDorYALBzjukFo7Z8934WvtnFs7XxjunG9fVVuPLzQALABJllXV0qHrmEutnbPnu/C1ds6tnS9M3jl7C0CSpAYZACRJapABYGHYf+gCBtDaOXu+C19r59za+cKEnbNjACRJapA9AJIkNcgAIElSgwwAkiQ1yAAgSVKDDACSJDXo/wM3wvaIAD9AwgAAAABJRU5ErkJggg==\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YX2rTZlcZbI5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}